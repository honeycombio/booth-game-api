
# To make this work, install the VSCode extension humau:rest-client
# then a little "Send Request" link appears above each GET and POST.

# also, copy .env.example to .env and populate a Honeycomb API key
# that the person would have entered into the quiz
# Otherwise you'll get internal server errors from our brilliant function

@AttendeeAPIKey = {{$dotenv ATTENDEE_API_KEY}}

POST {{hostname}}/api/queryData
X-Honeycomb-Api-Key: {{AttendeeAPIKey}}
X-Observaquiz-Execution-Id: 123

{
    "attendee_api_key": "{{AttendeeAPIKey}}",
    "dataset_slug": "observaquiz-bff",
    "query_name": "this is a test",
    "query": {
        "time_range": 600,
        "granularity": 0,
        "breakdowns": [],
        "calculations": [
            {
                "op": "COUNT"
            }
        ],
        "limit": 100
    }
}

### 

# Here is a real example from Question 

@AttendeeAPIKey = {{$dotenv ATTENDEE_API_KEY}} 

POST {{hostname}}/api/queryData
X-Honeycomb-Api-Key: {{AttendeeAPIKey}}
X-Observaquiz-Execution-Id: 123


{
"dataset_slug": "observaquiz-bff",
"attendee_api_key": "{{AttendeeAPIKey}}",
  "query": {
    "time_range": 600,
    "granularity": 0,
    "breakdowns": ["app.post_answer.question", "app.llm.input", "app.llm.output"],
    "calculations": [{ "op": "MAX", "column": "duration_ms" }],
    "filters": [
      { "column": "name", "op": "=", "value": "Ask LLM for Response" },
      { "column": "app.observaquiz.execution_id", "op": "=", "value": "1c3ebd9b-e2fe--a5b1-228d371f7d7f" }
    ],
    "orders": [{ "column": "duration_ms", "op": "MAX", "order": "descending" }],
    "havings": [],
    "limit": 1000
  },
  "query_name": "Slowest response from LLM"
}

### 
# Count trace spans
###

POST {{hostname}}/api/queryData
X-Honeycomb-Api-Key: {{AttendeeAPIKey}}
X-Observaquiz-Execution-Id: 123
Content-Type: application/json

{
  "query": {
    "time_range": 7200,
    "granularity": 0,
    "calculations": [
      {
        "op": "COUNT"
      }
    ],
    "filters": [
      {
        "column": "trace.trace_id",
        "op": "=",
        "value": "fdf357489dafbd986b84901308169ee7"
      }
    ],
    "breakdowns": [
      "trace.trace_id",
      "name"
    ]
  },
  "query_name": "Slowest response from LLM",
  "dataset_slug": "observaquiz-bff",
  "attendee_api_key": "cNBcvqfEUpDzaFhWWuT7yE"
}